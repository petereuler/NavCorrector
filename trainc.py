import os
import time
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import numpy as np
import torch.nn.functional as F
import matplotlib.pyplot as plt
import quaternion

from models.heading_classifier import (
    FeatureExtractor, RegressorHead,
    HeadingQuantizer, HeadingBinaryLoss,
    compute_bit_accuracy, compute_heading_mae, HeadingBinaryHead
)
from utils.training_utils import (
    len_loss,
    load_data_2d_oxiod,
    load_data_2d_selfmade,
    load_data_2d_ronin,
    plot_quantizer_analysis
)


# ======= å‚æ•°è®¾ç½® =======
window_size = 160
stride = 32
batch_size = 64
feat_dim = 64
output_dim_len = 1

# èˆªå‘è§’é‡åŒ–å‚æ•°
num_bits = 8  # å¿…é¡»æ˜¯ 4 çš„å€æ•°
num_bins = 2 ** num_bits
use_adaptive_quantization = False  # å¯ç”¨è‡ªé€‚åº”éå‡åŒ€é‡åŒ–
# è®¡ç®—è¾“å‡ºä½æ•°
output_bits = num_bits

# ä¼˜åŒ–å™¨å‚æ•°
lr = 1e-4
weight_decay = 1e-4
epochs = 200

# è®­ç»ƒæ¨¡å¼ï¼š'adaptive' (ä½™å¼¦é€€ç«+æ—©åœ) æˆ– 'fixed' (å›ºå®šå­¦ä¹ ç‡+å›ºå®šè½®æ•°)
train_mode = 'fixed'  # 'adaptive' or 'fixed'
early_stop_patience = 50  # ä»…åœ¨ adaptive æ¨¡å¼ä¸‹ç”Ÿæ•ˆ

# æ•°æ®å¢å¼º
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
dataset = "OXIOD"

# ä»ç¯å¢ƒå˜é‡è¯»å–
epochs = int(os.getenv('EPOCHS', epochs))


# ======= è®­ç»ƒå‡½æ•° =======

def train_length_model(extractor, regressor, train_loader, val_loader, ckpt_dir, curve_dir):
    """è®­ç»ƒæ­¥é•¿å›å½’æ¨¡å‹"""
    optimizer = optim.AdamW(
        list(extractor.parameters()) + list(regressor.parameters()), 
        lr=lr, weight_decay=weight_decay
    )
    
    # æ ¹æ®è®­ç»ƒæ¨¡å¼é€‰æ‹©å­¦ä¹ ç‡è°ƒåº¦å™¨
    if train_mode == 'fixed':
        scheduler = None  # å›ºå®šå­¦ä¹ ç‡
    else:
        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    
    ckpts = [os.path.join(ckpt_dir, f) for f in ["extractor_len.pth", "reg_len.pth"]]
    
    if os.path.exists(ckpts[0]) and os.path.exists(ckpts[1]):
        extractor.load_state_dict(torch.load(ckpts[0]))
        regressor.load_state_dict(torch.load(ckpts[1]))
        print("[Length] å‘ç°å·²æœ‰æœ€ä½³æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ")
        return
    
    best_loss = float('inf')
    train_curve = []
    val_curve = []
    no_improve = 0
    
    mode_str = "Fixed LR" if train_mode == 'fixed' else "Adaptive (Cosine+EarlyStop)"
    print(f">>> å¼€å§‹è®­ç»ƒæ­¥é•¿æ¨¡å‹ (Regression, {mode_str})")
    for ep in range(epochs):
        t0 = time.time()
        extractor.train()
        regressor.train()
        total = 0.0
        cnt = 0
        
        for xb, yb_len, yb_head in train_loader:
            feat = extractor(xb)
            pred = regressor(feat)
            loss = len_loss(pred, yb_len)
                
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(extractor.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(regressor.parameters(), 1.0)
            optimizer.step()
            
            bs = xb.size(0)
            total += loss.item() * bs
            cnt += bs
        
        if scheduler is not None:
            scheduler.step()
        train_loss = total / max(cnt, 1)
        
        # éªŒè¯
        extractor.eval()
        regressor.eval()
        vtotal = 0.0
        vcnt = 0
        with torch.no_grad():
            for xb, yb_len, _ in val_loader:
                feat = extractor(xb)
                pred = regressor(feat)
                loss = len_loss(pred, yb_len)
                bs = xb.size(0)
                vtotal += loss.item() * bs
                vcnt += bs
        val_loss = vtotal / max(vcnt, 1)
        
        train_curve.append(train_loss)
        val_curve.append(val_loss)
        
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(extractor.state_dict(), ckpts[0])
            torch.save(regressor.state_dict(), ckpts[1])
            no_improve = 0
        else:
            no_improve += 1
        
        # æ—©åœæœºåˆ¶ä»…åœ¨ adaptive æ¨¡å¼ä¸‹ç”Ÿæ•ˆ
        if train_mode == 'adaptive' and no_improve >= early_stop_patience:
            print(f"  Early stopping at epoch {ep+1}")
            break
            
        if (ep + 1) % 10 == 0 or ep == 0:
            current_lr = scheduler.get_last_lr()[0] if scheduler else lr
            print(f"[Length] Ep {ep+1}/{epochs} train={train_loss:.5f} val={val_loss:.5f} "
                  f"lr={current_lr:.2e} time={time.time()-t0:.1f}s")

    plt.figure()
    plt.plot(train_curve, label='train')
    plt.plot(val_curve, label='val')
    plt.title(f'Length Loss (MSE) - {mode_str}')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(os.path.join(curve_dir, 'curve_length.png'))
    plt.close()


def train_heading_classifier(extractor, head, train_loader, val_loader,
                             ckpt_dir, curve_dir, quantizer):
    """è®­ç»ƒèˆªå‘åˆ†ç±»æ¨¡å‹ (Binary Output + Soft Decode Validation)"""
    optimizer = optim.AdamW(
        list(extractor.parameters()) + list(head.parameters()),
        lr=lr, weight_decay=weight_decay
    )

    if train_mode == 'fixed':
        scheduler = None
    else:
        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)

    # å®šä¹‰åŸºç¡€æƒé‡
    base_circular_weight = 1  # å»ºè®®ç¨å¾®æé«˜ä¸€ç‚¹æƒé‡ï¼Œå› ä¸ºç°åœ¨Lossæ˜¯å¯å¯¼çš„

    # åˆå§‹åŒ– Loss (åˆå§‹æƒé‡è®¾ä¸º 0)
    criterion = HeadingBinaryLoss(
        num_bits=num_bits,
        use_gray_code=True,
        quantizer=quantizer,
        circular_weight=0.0
    )
    
    ckpts = [os.path.join(ckpt_dir, f) for f in ["extractor_head_cls.pth", "cls_head.pth"]]

    if os.path.exists(ckpts[0]) and os.path.exists(ckpts[1]):
        extractor.load_state_dict(torch.load(ckpts[0]))
        head.load_state_dict(torch.load(ckpts[1]))
        print("[Heading] å‘ç°å·²æœ‰æœ€ä½³æ¨¡å‹ï¼Œè·³è¿‡è®­ç»ƒ")
        return

    best_mae = float('inf')
    train_curve = []
    val_curve = []
    val_mae_curve = []
    no_improve = 0

    mode_str = "Fixed LR" if train_mode == 'fixed' else "Adaptive (Cosine+EarlyStop)"
    print(f">>> å¼€å§‹è®­ç»ƒèˆªå‘æ¨¡å‹ (Binary Loss, Soft Val, {mode_str})")

    for ep in range(epochs):
        t0 = time.time()

        # Loss Warm-up: å‰5è½®ä¸åŠ å‡ ä½•çº¦æŸï¼Œä¹‹åçº¿æ€§å¢åŠ æˆ–å›ºå®š
        if ep < 5:
            current_geo_weight = 0.0
        else:
            current_geo_weight = base_circular_weight
        criterion.circular_weight = current_geo_weight

        extractor.train()
        head.train()
        total_loss = 0.0
        total_bce = 0.0
        total_geo = 0.0
        cnt = 0

        for xb, _, yb_head in train_loader:
            feat = extractor(xb)
            logits = head(feat)

            # è·å–è¯¦ç»† Loss
            loss, details = criterion(logits, yb_head, return_details=True)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(extractor.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(head.parameters(), 1.0)
            optimizer.step()

            bs = xb.size(0)
            total_loss += loss.item() * bs
            total_bce += details['bce'] * bs
            total_geo += details['geo'] * bs
            cnt += bs

        if scheduler is not None:
            scheduler.step()

        avg_train_loss = total_loss / max(cnt, 1)
        avg_bce = total_bce / max(cnt, 1)
        avg_geo = total_geo / max(cnt, 1)

        # éªŒè¯ Loop
        extractor.eval()
        head.eval()
        vtotal = 0.0
        vcnt = 0
        all_logits = []
        all_targets = []

        with torch.no_grad():
            for xb, _, yb_head in val_loader:
                feat = extractor(xb)
                logits = head(feat)
                loss = criterion(logits, yb_head) # éªŒè¯é›†Lossåªä½œå‚è€ƒ

                bs = xb.size(0)
                vtotal += loss.item() * bs
                vcnt += bs

                all_logits.append(logits)
                all_targets.append(yb_head)

        val_loss = vtotal / max(vcnt, 1)

        # ==========================================
        # å…³é”®ä¿®æ”¹ï¼šéªŒè¯é›†ä½¿ç”¨ Soft Decoding è®¡ç®— MAE
        # ==========================================
        all_logits = torch.cat(all_logits, dim=0)
        all_targets = torch.cat(all_targets, dim=0)

        # ä½¿ç”¨æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚å®ç°çš„ decode_soft_expectation
        pred_heading_soft = quantizer.decode_soft_expectation(all_logits)

        # è®¡ç®— MAE (Pred å’Œ Target éƒ½æ˜¯ Tensor ä¸”åœ¨ GPU ä¸Š)
        mae = compute_heading_mae(pred_heading_soft, all_targets)
        
        train_curve.append(avg_train_loss)
        val_curve.append(val_loss)
        val_mae_curve.append(mae.item())

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if mae.item() < best_mae:
            best_mae = mae.item()
            torch.save(extractor.state_dict(), ckpts[0])
            torch.save(head.state_dict(), ckpts[1])
            no_improve = 0
        else:
            no_improve += 1

        if train_mode == 'adaptive' and no_improve >= early_stop_patience:
            print(f"  Early stopping at epoch {ep+1}")
            break

        if (ep + 1) % 5 == 0 or ep == 0:
            current_lr = scheduler.get_last_lr()[0] if scheduler else lr
            # æ‰“å°åŒ…å«åŒæµ Loss çš„ä¿¡æ¯
            print(f"[Heading Ep {ep+1}] "
                  f"Loss: {avg_train_loss:.4f} (BCE_A:{avg_bce:.3f}, BCE_B:{avg_bce:.3f}, Geo:{avg_geo:.3f}, W:{current_geo_weight}) "
                  f"| Val MAE: {np.degrees(mae.item()):.2f}Â° "
                  f"| Time: {time.time()-t0:.1f}s")

    # ç»˜åˆ¶æ›²çº¿
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    axes[0, 0].plot(train_curve, label='train')
    axes[0, 0].plot(val_curve, label='val')
    axes[0, 0].set_title('Binary Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].legend()
    
    # Validation Accuracy plot removed (not applicable for binary) or replaced
    axes[0, 1].text(0.5, 0.5, "Binary Output Mode\nAccuracy Metric N/A", ha='center')
    axes[0, 1].axis('off')
    
    axes[1, 0].plot([np.degrees(m) for m in val_mae_curve])
    axes[1, 0].set_title('Validation MAE (Soft Decoding)')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('MAE (deg)')
    
    # æœ€ä½³ MAE æ ‡è®°
    best_idx = np.argmin(val_mae_curve)
    axes[1, 0].scatter([best_idx], [np.degrees(val_mae_curve[best_idx])], 
                       color='green', s=100, zorder=5, label=f'Best: {np.degrees(val_mae_curve[best_idx]):.2f}deg')
    axes[1, 0].legend()
    
    # ç»˜åˆ¶ bin å®½åº¦åˆ†å¸ƒï¼ˆä»…è‡ªé€‚åº”é‡åŒ–ï¼‰
    if use_adaptive_quantization and quantizer.fitted:
        bin_widths = np.diff(quantizer.bin_edges)
        axes[1, 1].bar(range(len(bin_widths)), np.degrees(bin_widths), alpha=0.7)
        axes[1, 1].axhline(y=np.degrees(2*np.pi/num_bins), color='r', linestyle='--', 
                          label=f'Uniform: {np.degrees(2*np.pi/num_bins):.2f}deg')
        axes[1, 1].set_title('Bin Width Distribution (Adaptive)')
        axes[1, 1].set_xlabel('Bin Index')
        axes[1, 1].set_ylabel('Width (deg)')
        axes[1, 1].legend()
    else:
        axes[1, 1].text(0.5, 0.5, f"Best MAE: {np.degrees(best_mae):.2f}deg\n"
                        f"Num Bits: {num_bits}\n",
                        ha='center', va='center', fontsize=14,
                        transform=axes[1, 1].transAxes)
        axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(curve_dir, 'curve_heading.png'))
    plt.close()
    
    print(f"\næœ€ä½³éªŒè¯ MAE: {np.degrees(best_mae):.2f}Â°")


def main():
    project_dir = "/home/admin407/code/zyshe/Corrector"
    data_root = os.path.join(project_dir, "OXIOD")
    selfmade_root = os.path.join(project_dir, "SELFMADE")
    ronin_root = os.path.join(project_dir, "RONIN")
    ckpt_dir = os.path.join(project_dir, "checkpoints_cls")
    os.makedirs(ckpt_dir, exist_ok=True)
    curve_dir = os.path.join(project_dir, "output", f"trainc_{time.strftime('%Y%m%d_%H%M%S')}")
    os.makedirs(curve_dir, exist_ok=True)
    
    quantizer_path = os.path.join(ckpt_dir, "quantizer.json")

    print("="*60)
    print("èˆªå‘è§’é‡åŒ–åˆ†ç±»è®­ç»ƒï¼ˆè‡ªé€‚åº”éå‡åŒ€é‡åŒ–ç‰ˆï¼‰")
    print("="*60)
    print(f"  ä½æ•°: {num_bits} bits -> {num_bins} bins")
    print(f"  è¾“å‡ºä½æ•°: {output_bits} bits")
    print(f"  è‡ªé€‚åº”é‡åŒ–: {use_adaptive_quantization}")
    print(f"  æŸå¤±å‡½æ•°: HeadingBinaryLoss (äºŒè¿›åˆ¶ç¼–ç )")
    print(f"  è®­ç»ƒæ¨¡å¼: {train_mode} ({'å›ºå®šå­¦ä¹ ç‡+å›ºå®šè½®æ•°' if train_mode == 'fixed' else 'ä½™å¼¦é€€ç«+æ—©åœ'})")
    print(f"  å­¦ä¹ ç‡: {lr}, æƒé‡è¡°å‡: {weight_decay}, è½®æ•°: {epochs}")
    print("="*60)

    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    if dataset == "SELFMADE" and os.path.isdir(selfmade_root):
        x_tr, ylen_tr, yhead_tr, x_va, ylen_va, yhead_va = load_data_2d_selfmade(selfmade_root, device, window_size, stride)
    elif dataset == "RONIN" and os.path.isdir(ronin_root):
        x_tr, ylen_tr, yhead_tr, x_va, ylen_va, yhead_va = load_data_2d_ronin(ronin_root, device, window_size, stride)
    else:
        x_tr, ylen_tr, yhead_tr, x_va, ylen_va, yhead_va = load_data_2d_oxiod(data_root, device, window_size, stride)
        
    print(f"è®­ç»ƒé›†: {x_tr.shape[0]} æ ·æœ¬")
    print(f"éªŒè¯é›†: {x_va.shape[0]} æ ·æœ¬")
    
    # æ‰“å°èˆªå‘è§’åˆ†å¸ƒ
    head_tr_np = yhead_tr.cpu().numpy().flatten()
    print(f"èˆªå‘è§’èŒƒå›´: [{np.degrees(head_tr_np.min()):.1f}Â°, {np.degrees(head_tr_np.max()):.1f}Â°]")
    print(f"èˆªå‘è§’æ ‡å‡†å·®: {np.degrees(head_tr_np.std()):.1f}Â°")
    print(f"èˆªå‘è§’ä¸­ä½æ•°: {np.degrees(np.median(head_tr_np)):.1f}Â°")
    
    # åˆå§‹åŒ–é‡åŒ–å™¨
    print("\nğŸ“ åˆå§‹åŒ–é‡åŒ–å™¨...")
    quantizer = HeadingQuantizer(
        num_bins=num_bins,
        use_gray_code=True
    )
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¿å­˜çš„é‡åŒ–å™¨
    if os.path.exists(quantizer_path):
        print(f"  å‘ç°å·²æœ‰é‡åŒ–å™¨ï¼Œä» {quantizer_path} åŠ è½½")
        quantizer.load(quantizer_path)
    else:
        # ä»…ä½¿ç”¨è®­ç»ƒé›†æ•°æ®æ‹Ÿåˆé‡åŒ–å™¨ï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰
        print("  ä½¿ç”¨è®­ç»ƒé›†æ•°æ®æ‹Ÿåˆé‡åŒ–å™¨...")
        # æ–­è¨€ç¡®ä¿åªä½¿ç”¨è®­ç»ƒé›†
        assert head_tr_np.shape[0] == len(x_tr), "é‡åŒ–å™¨æ‹Ÿåˆæ•°æ®å¿…é¡»ä»…åŒ…å«è®­ç»ƒé›†"
        quantizer.fit(head_tr_np)
        quantizer.save(quantizer_path)
    
    # ç»˜åˆ¶é‡åŒ–å™¨åˆ†æå›¾
    plot_quantizer_analysis(quantizer, head_tr_np, curve_dir, num_bins)
    
    train_dataset = TensorDataset(x_tr, ylen_tr, yhead_tr)
    val_dataset = TensorDataset(x_va, ylen_va, yhead_va)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)

    # æ¨¡å‹åˆå§‹åŒ–
    in_ch = x_tr.shape[-1]
    
    # æ­¥é•¿æ¨¡å‹
    extractor_len = FeatureExtractor(in_channels=in_ch, feat_dim=feat_dim).to(device)
    reg_len = RegressorHead(feat_dim, output_dim_len).to(device)
    
    # èˆªå‘æ¨¡å‹ï¼ˆä½¿ç”¨æ”¹è¿›çš„äºŒè¿›åˆ¶åˆ†ç±»å¤´ï¼‰
    extractor_head = FeatureExtractor(in_channels=in_ch, feat_dim=feat_dim).to(device)
    # æ³¨æ„ï¼šè¿™é‡Œè¾“å‡ºç»´åº¦æ˜¯ output_bits
    head = HeadingBinaryHead(feat_dim, num_bits=output_bits, hidden_dim=256, dropout=0.3).to(device)

    # è®­ç»ƒ
    print("\nğŸ¯ è®­ç»ƒæ­¥é•¿æ¨¡å‹")
    train_length_model(extractor_len, reg_len, train_loader, val_loader, ckpt_dir, curve_dir)
    
    print("\nğŸ¯ è®­ç»ƒèˆªå‘åˆ†ç±»æ¨¡å‹ (Binary)")
    train_heading_classifier(extractor_head, head, train_loader, val_loader, 
                            ckpt_dir, curve_dir, quantizer)
    
    print("\nâœ… è®­ç»ƒå®Œæˆ")
    print(f"   æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: {ckpt_dir}")
    print(f"   é‡åŒ–å™¨ä¿å­˜åœ¨: {quantizer_path}")
    print(f"   è®­ç»ƒæ›²çº¿ä¿å­˜åœ¨: {curve_dir}")


if __name__ == "__main__":
    main()
